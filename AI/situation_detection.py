import cv2
import time
import json
import base64
import numpy as np
import paho.mqtt.client as mqtt
from ultralytics import YOLO
import threading
from pose_api import start_pose_api_server, update_pose
from datetime import datetime
import subprocess
import os

def start_rtsp_server():
    if not os.path.exists("/home/ssafy/mediamtx"):
        raise FileNotFoundError("âŒ mediamtx ì‹¤í–‰ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    print("âœ… RTSP ì„œë²„(mediamtx) ì‹¤í–‰ ì¤‘...")
    return subprocess.Popen(["/home/ssafy/mediamtx"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)

def start_ffmpeg_stream():
    ffmpeg_cmd = [
        "ffmpeg",
        "-f", "v4l2",
        "-input_format", "yuyv422",
        "-framerate", "30",
        "-video_size", "1280x720",
        "-i", "/dev/video0",
        "-pix_fmt", "yuv420p",
        "-c:v", "libx264",
        "-preset", "ultrafast",
        "-tune", "zerolatency",
        "-profile:v", "baseline",
        "-level:v", "3.1",
        "-f", "rtsp",
        "rtsp://localhost:8554/mystream"
    ]
    print("ğŸ¥ FFmpeg ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì¤‘...")
    return subprocess.Popen(ffmpeg_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

# COCO 17-kpt ì¸ë±ìŠ¤ (Ultralytics ê¸°ì¤€)
NOSE, LEYE, REYE, LEAR, REAR, LSH, RSH, LEL, REL, LWR, RWR, LHIP, RHIP, LKNE, RKNE, LANK, RANK = range(17)

def _mid(p1, p2):
    return (p1 + p2) / 2.0

def lie_score_from_keypoints(kpts_xy: np.ndarray, bbox, img_h: int):
    """
    kpts_xy: (17,2) ndarray with [x,y] in pixels
    bbox: [x1,y1,x2,y2] in pixels
    img_h: image height (for normalization)
    returns: (lie_score: float 0~1, detail: dict)
    """
    # 1) ìƒì²´ ë°©í–¥ ìˆ˜í‰ì„± (ì–´ê¹¨â†”ì—‰ë©ì´ ì¤‘ì‹¬ ë²¡í„°)
    sh_c = _mid(kpts_xy[LSH], kpts_xy[RSH])
    hip_c = _mid(kpts_xy[LHIP], kpts_xy[RHIP])
    v = hip_c - sh_c
    v_norm = np.linalg.norm(v) + 1e-6
    v_unit = v / v_norm
    vertical = np.array([0.0, 1.0])
    verticality = abs(float(np.dot(v_unit, vertical)))  # 1=ìˆ˜ì§, 0=ìˆ˜í‰
    horizontal_score = 1.0 - verticality

    # 2) ë°•ìŠ¤ ê°€ë¡œ/ì„¸ë¡œ ë¹„
    x1, y1, x2, y2 = bbox
    w, h = max(1.0, x2 - x1), max(1.0, y2 - y1)
    aspect = w / h
    aspect_score = float(np.tanh(aspect - 1.2))  # 1.2ë¶€í„° ê°€ì 

    # 3) ê´€ì ˆì˜ y-ìˆœì„œê°€ ê¹¨ì¡ŒëŠ”ì§€
    ys = {
        "ankle": np.mean([kpts_xy[LANK,1], kpts_xy[RANK,1]]),
        "knee":  np.mean([kpts_xy[LKNE,1], kpts_xy[RKNE,1]]),
        "hip":   hip_c[1],
        "shoulder": sh_c[1],
        "nose": kpts_xy[NOSE,1],
    }
    order = ["ankle","knee","hip","shoulder","nose"]
    inversions = 0
    for i in range(len(order)-1):
        if not (ys[order[i]] > ys[order[i+1]]):  # ì„œ ìˆìœ¼ë©´ ankle>knee>... ì„±ë¦½
            inversions += 1
    order_score = inversions / 4.0

    # 4) y-ë¶„ì‚° (ëˆ•ê¸°ë©´ ì—¬ëŸ¬ ê´€ì ˆ yê°€ ë¹„ìŠ· â†’ ë¶„ì‚°â†“)
    key_idxs = [NOSE, LSH, RSH, LHIP, RHIP, LKNE, RKNE, LANK, RANK]
    yvals = kpts_xy[key_idxs,1]
    yspread = (float(np.max(yvals)) - float(np.min(yvals))) / (img_h + 1e-6)
    spread_score = 1.0 - float(np.clip(yspread / 0.5, 0, 1))

    lie_score = 0.45*horizontal_score + 0.25*aspect_score + 0.20*order_score + 0.10*spread_score
    return float(lie_score), {
        "horizontal": float(horizontal_score),
        "aspect": float(aspect_score),
        "order": float(order_score),
        "spread": float(spread_score),
        "aspect_raw": float(aspect)
    }

# ì•Œë¦¼ì„ ë³´ë‚¼ íŠ¹ì • í´ë˜ìŠ¤ ì´ë¦„ ì •ì˜
ALERT_CLASSES = [
    "MATERIAL_COLLAPSE",     # ì ì¬ ë¬¼ë¥˜ ë¶•ê´´
    "SMOKING_VIOLATION",     # ì‘ì—…ì¥ ë‚´ í¡ì—°
    "NOT_WEAR_WORKER",       # ì•ˆì „ì¥ë¹„ ë¯¸ì°©ìš©
    "PERSON_FALL"
]

detection_states = {
    cls_name: {'start_time': None, 'alert_sent': False, 'alert_sent_time': None}
    for cls_name in ALERT_CLASSES
}

ALERT_DURATION_THRESHOLD = 0.5  
RESET_ALERT_AFTER = 60  
# --- lie/fall íŒì • ë³´ê°•ìš© ìƒìˆ˜ ---
KP_CONF_THRES = 0.35       # í‚¤í¬ì¸íŠ¸ ìµœì†Œ ì‹ ë¢°ë„
MIN_VISIBLE_KPTS = 8        # ì „ì²´ ìµœì†Œ ê°€ì‹œ í‚¤í¬ì¸íŠ¸ ìˆ˜
EDGE_MARGIN = 8             # í”„ë ˆì„ ê²½ê³„ ì—¬ìœ (px)
REQUIRE_LOWER_BODY_FOR_FALL = True  # í•˜ì²´(ë¬´ë¦/ë°œëª©) ì¼ë¶€ë¼ë„ ë³´ì—¬ì•¼ 'FALL' í—ˆìš©
LIE_THRESHOLD = 0.55     # ì´ ì´ìƒì´ë©´ 'ëˆ•ê¸°'ë¡œ ê°„ì£¼ (í™˜ê²½ë³„ë¡œ 0.5~0.65 ì‚¬ì´ íŠœë‹ ê¶Œì¥)
MIN_HORIZONTAL_SCORE = 0.35 # ìˆ˜í‰ì„± ìµœì†Œ ìš”ê±´
MIN_BOX_AREA = 80*80     # ë„ˆë¬´ ì‘ì€ ì¸ë¬¼ ë°•ìŠ¤ëŠ” ë¬´ì‹œ(ì›ê±°ë¦¬ ë…¸ì´ì¦ˆ ë°©ì§€)

# MQTT ì„¤ì •
BROKER = "192.168.100.141"
PORT = 1883
AMR_SERIAL = "AMR001"   
TOPIC = f"alert"

# client = mqtt.Client()
# client.connect(BROKER, PORT, 60)

SITUATION_MAPPING = {
    "MATERIAL_COLLAPSE": "COLLAPSE",
    "SMOKING_VIOLATION": "SMOKE", 
    "NOT_WEAR_WORKER": "EQUIPMENT",
    "PERSON_FALL": "FALL"    # â¬…ï¸ ì¶”ê°€
}

# robot_poseë¥¼ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸í•˜ê³  ì´ˆê¸°í™”
robot_pose = {'x': None, 'y': None}

# class PoseSubscriber(Node):
#     def __init__(self):
#         super().__init__('pose_subscriber')
        
        # /amcl_pose í† í”½ ë°œí–‰ìì˜ QoS í”„ë¡œí•„ê³¼ ì¼ì¹˜í•˜ë„ë¡ ì„¤ì •
        # qos_profile = QoSProfile(depth=10)
        # qos_profile.reliability = ReliabilityPolicy.RELIABLE
        # qos_profile.history = HistoryPolicy.KEEP_LAST
        # qos_profile.durability = DurabilityPolicy.TRANSIENT_LOCAL

        # self.subscription = self.create_subscription(
        #     PoseWithCovarianceStamped,
        #     '/amcl_pose',  # Localizationì—ì„œ í¼ë¸”ë¦¬ì‹œí•˜ëŠ” í† í”½
        #     self.listener_callback,
        #     qos_profile) # QoS í”„ë¡œí•„ ì ìš©

#     def listener_callback(self, msg):
#         global robot_pose
#         x = msg.pose.pose.position.x
#         y = msg.pose.pose.position.y
#         robot_pose['x'] = x
#         robot_pose['y'] = y
#         update_pose(x, y)  # <--- API ì„œë²„ì— í˜„ì¬ ì¢Œí‘œ ì—…ë°ì´íŠ¸
#         self.get_logger().info(f"Received pose: x={x}, y={y}")


# def start_ros2_node():
#     rclpy.init()
#     node = PoseSubscriber()
#     rclpy.spin(node)
#     node.destroy_node()
#     rclpy.shutdown()

# ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰
# ros_thread = threading.Thread(target=start_ros2_node, daemon=True)
# ros_thread.start()

# API ì„œë²„ ì‹¤í–‰ (ë¡œì»¬ + ì™¸ë¶€ì—ì„œ ì¢Œí‘œ ì ‘ê·¼ ê°€ëŠ¥)
# start_pose_api_server()

def send_alert_to_backend(situation, image_base64="", x=None, y=None, detail=""):
    """
    AIì—ì„œ Backendë¡œ ì•Œë¦¼ ì „ì†¡
    
    Args:
        situation (str): ìƒí™© ("COLLAPSE", "SMOKE", "EQUIPMENT")
        image_base64 (str): Base64ë¡œ ì¸ì½”ë”©ëœ ì´ë¯¸ì§€
        x (str): X ì¢Œí‘œ ë˜ëŠ” êµ¬ì—­ëª…
        y (str): Y ì¢Œí‘œ ë˜ëŠ” êµ¬ì—­ëª…
        detail (str): ìƒì„¸ ì •ë³´
    """
    global robot_pose
    if x is None or y is None:
        x_val = robot_pose['x'] if robot_pose['x'] is not None else 0.0
        y_val = robot_pose['y'] if robot_pose['y'] is not None else 0.0
    else:
        x_val = x
        y_val = y
    current_time_str = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")

    message = {
        "serial" : "AMR001",
        "case": situation,
        "image": image_base64,
        "x": x_val,
        "y": y_val,
        "timeStamp": current_time_str
    }

    # try:
    #     client.publish(TOPIC, json.dumps(message))
    #     print(f"[AI -> Backend] ìƒí™©: {situation}, ìƒì„¸: {detail}, ë¡œë´‡ ìœ„ì¹˜: x={x_val}, y={y_val}")
    #     return True
    # except Exception as e:
    #     print(f"[ERROR] ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
    #     return False

# ì´ë¯¸ì§€ í”„ë ˆì„ì„ Base64ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def encode_frame_to_base64(frame):
    """ì´ë¯¸ì§€ í”„ë ˆì„ì„ Base64ë¡œ ì¸ì½”ë”©"""
    try:
        _, buffer = cv2.imencode('.jpg', frame)
        return base64.b64encode(buffer).decode("utf-8")
    except Exception as e:
        print(f"[ERROR] ì´ë¯¸ì§€ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
        return ""

def get_situation_detail(situation):
    """ìƒí™©ë³„ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
    detail_mapping = {
        "COLLAPSE": "ì ì¬ ë¬¼ë¥˜ê°€ ë¶•ê´´ë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.",
        "SMOKE": "ì‘ì—…ì¥ ë‚´ì—ì„œ í¡ì—°ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì•ˆì „ ê·œì •ì„ í™•ì¸í•˜ì„¸ìš”.",
        "EQUIPMENT": "ì•ˆì „ì¥ë¹„ë¥¼ ì°©ìš©í•˜ì§€ ì•Šì€ ì‘ì—…ìê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "FALL": "ì“°ëŸ¬ì ¸ ìˆëŠ” ì‘ì—…ìê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì‘ê¸‰ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
    }
    return detail_mapping.get(situation, "ì•Œ ìˆ˜ ì—†ëŠ” ìƒí™©ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

def main():
    # â–¶ï¸ RTSP ì„œë²„ ë° ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
    # try:
    #     rtsp_proc = start_rtsp_server()
    #     time.sleep(2)  # ì„œë²„ ì‹œì‘ ëŒ€ê¸°

    #     ffmpeg_proc = start_ffmpeg_stream()
    #     time.sleep(2)  # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ëŒ€ê¸°
    # except Exception as e:
    #     print(f"RTSP ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    #     return
    # ì›¹ìº  ì´ˆê¸°í™”
    cap = cv2.VideoCapture(0)
    # cap = cv2.VideoCapture("/dev/video0")

    if not cap.isOpened():
        print("ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # YOLOv8 ëª¨ë¸ ë¡œë“œ
    try:
        model = YOLO("epoch40.pt").to("cuda")
        pose_model = YOLO("yolo11n-pose.pt").to("cuda")
        print("YOLO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"YOLO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    print(f"AI ì•Œë¦¼ ì‹œìŠ¤í…œ ì‹œì‘ - AMR: {AMR_SERIAL}")
    print(f"MQTT í† í”½: {TOPIC}")
    print("ê°ì§€ ëŒ€ìƒ:", ALERT_CLASSES)

    try:
        while True:
            # ros_threadê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ, main ë£¨í”„ëŠ” ì´ë¯¸ì§€ ì²˜ë¦¬ì— ì§‘ì¤‘
            ret, frame = cap.read()
            if not ret:
                print("í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            current_time = time.time()
            detected_classes_in_current_frame = set()
            img_h, img_w = frame.shape[:2]
            
            # YOLO ëª¨ë¸ë¡œ ê°ì²´ ê°ì§€ ìˆ˜í–‰
            try:
                results = model(frame, stream=True)

                # ê°ì§€ ê²°ê³¼ ì²˜ë¦¬ ë° í™”ë©´ì— ê·¸ë¦¬ê¸°
                for r in results:
                    boxes = r.boxes
                    for box in boxes:
                        # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        
                        # ì‹ ë¢°ë„
                        confidence = float(box.conf[0])
                        
                        # í´ë˜ìŠ¤ IDì™€ ì´ë¦„
                        class_id = int(box.cls[0])
                        class_name = model.names[class_id]

                        # ì•Œë¦¼ ëŒ€ìƒ í´ë˜ìŠ¤ì¸ ê²½ìš°ë§Œ ì²˜ë¦¬
                        if class_name in ALERT_CLASSES and confidence > 0.7:
                            detected_classes_in_current_frame.add(class_name)
                            
                            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œ)
                            color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰ (BGR)
                            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                            
                            # ë ˆì´ë¸” í‘œì‹œ
                            label = f"{class_name} {confidence:.2f}"
                            cv2.putText(frame, label, (x1, y1 - 10), 
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

            except Exception as e:
                print(f"YOLO ê°ì§€ ì˜¤ë¥˜: {e}")

            try:
                pose_res = pose_model(frame, verbose=False)[0]
                if pose_res.keypoints is not None and pose_res.boxes is not None:
                    kpts_xy_all = pose_res.keypoints.xy
                    boxes_all = pose_res.boxes.xyxy

                    # conf(ê°€ì‹œì„±) ì•ˆì „ ì¶”ì¶œ
                    conf_all = None
                    if hasattr(pose_res.keypoints, "conf") and pose_res.keypoints.conf is not None:
                        conf_all = pose_res.keypoints.conf  # (n,17)
                    else:
                        # fallback: dataê°€ (n,17,3) í˜•íƒœë©´ ë§ˆì§€ë§‰ ì±„ë„ì´ conf
                        if hasattr(pose_res.keypoints, "data"):
                            data = pose_res.keypoints.data  # (n,17,2 or 3)
                            if data is not None and data.shape[-1] == 3:
                                conf_all = data[..., 2]

                    for i, (kpts, b) in enumerate(zip(kpts_xy_all, boxes_all)):
                        kpts_xy = kpts.detach().cpu().numpy()        # (17,2)
                        bbox = b.detach().cpu().numpy()              # [x1,y1,x2,y2]
                        x1, y1, x2, y2 = bbox.astype(int)
                        area = max(1, (x2-x1)*(y2-y1))
                        if area < MIN_BOX_AREA:
                            continue

                        # ê°€ì‹œì„± ë§ˆìŠ¤í¬ ê³„ì‚°
                        if conf_all is not None:
                            kconf = conf_all[i].detach().cpu().numpy()  # (17,)
                            vis = kconf >= KP_CONF_THRES
                        else:
                            # conf ì •ë³´ê°€ ì—†ìœ¼ë©´ ëŠìŠ¨í•˜ê²Œ ëª¨ë‘ ë³´ì¸ ê²ƒìœ¼ë¡œ ê°„ì£¼
                            vis = np.ones(17, dtype=bool)

                        # ê´€ì ˆ ì¸ë±ìŠ¤
                        NOSE, LSH, RSH, LHIP, RHIP, LKNE, RKNE, LANK, RANK = 0, 5, 6, 11, 12, 13, 14, 15, 16

                        visible_cnt = int(vis.sum())
                        has_shoulders = bool(vis[LSH] and vis[RSH])
                        has_hips = bool(vis[LHIP] and vis[RHIP])
                        has_lower = bool(vis[LKNE] or vis[RKNE] or vis[LANK] or vis[RANK])

                        # í”„ë ˆì„ ê²½ê³„ í¬ë¡­ ì—¬ë¶€
                        crop_y = (y1 <= EDGE_MARGIN) or (y2 >= frame.shape[0]-EDGE_MARGIN)

                        # ===== ê°€ë“œë ˆì¼: ë¶€ë¶„ í¬ë¡­/ê°€ì‹œì„± ì²´í¬ =====
                        # 1) ìµœì†Œ ê°€ì‹œ í‚¤í¬ì¸íŠ¸ ìˆ˜
                        if visible_cnt < MIN_VISIBLE_KPTS:
                            continue
                        # 2) ì–´ê¹¨ëŠ” ë°˜ë“œì‹œ ë‘˜ ë‹¤ í•„ìš” (ìƒì²´ ë°©í–¥ ì¶”ì •ì˜ ì‹ ë¢°ì„±)
                        if not has_shoulders:
                            continue
                        # 3) í•˜ì²´ ì¼ë¶€ë¼ë„ ë³´ì´ê±°ë‚˜, ìµœì†Œí•œ ì—‰ë©ì´+ì–´ê¹¨ ì¡°í•©ì€ ìˆì–´ì•¼ í•¨
                        if REQUIRE_LOWER_BODY_FOR_FALL:
                            if not (has_lower or has_hips):
                                continue
                        else:
                            if not (has_hips or has_lower):
                                continue
                        # 4) í”„ë ˆì„ ìƒ/í•˜ë‹¨ì— ê±¸ë¦¬ë©´ì„œ í•˜ì²´ê°€ ì•ˆ ë³´ì´ë©´ ë³´ë¥˜(ì˜¤íƒ ë°©ì§€)
                        if crop_y and not has_lower:
                            continue

                        # ê±°ë¥´ê¸° í†µê³¼í–ˆìœ¼ë©´ ì ìˆ˜ ê³„ì‚°
                        lie_score, detail = lie_score_from_keypoints(kpts_xy, bbox, frame.shape[0])

                        # ìˆ˜í‰ì„± ìµœì†Œ ìš”ê±´ ì¶”ê°€(ë¶€ë¶„í¬ë¡­ ì‹œ aspectë§Œìœ¼ë¡œ ëˆ•ê¸°ë¡œ ê°€ëŠ” ê²ƒ ë°©ì§€)
                        if lie_score >= LIE_THRESHOLD and detail.get("horizontal", 0.0) >= MIN_HORIZONTAL_SCORE:
                            detected_classes_in_current_frame.add("PERSON_FALL")

                            # ì‹œê°í™”
                            color = (0, 165, 255)
                            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                            cv2.putText(frame, f"PERSON_FALL {lie_score:.2f}",
                                        (x1, max(0, y1-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

            except Exception as e:
                print(f"Pose ê³„ì‚° ì˜¤ë¥˜: {e}")
            # ê° ì•Œë¦¼ ëŒ€ìƒ í´ë˜ìŠ¤ì— ëŒ€í•œ íƒì§€ ìƒíƒœ ì—…ë°ì´íŠ¸ ë° ì•Œë¦¼ ì „ì†¡
            for cls_name in ALERT_CLASSES:
                if cls_name in detected_classes_in_current_frame:
                    # í´ë˜ìŠ¤ê°€ í˜„ì¬ íƒì§€ë¨
                    if detection_states[cls_name]['start_time'] is None:
                        # ìƒˆë¡œ íƒì§€ëœ ê²½ìš°, ì‹œì‘ ì‹œê°„ ê¸°ë¡
                        detection_states[cls_name]['start_time'] = current_time
                        detection_states[cls_name]['alert_sent'] = False
                    
                    # íƒì§€ ì§€ì† ì‹œê°„ ê³„ì‚°
                    duration = current_time - detection_states[cls_name]['start_time']

                    # 0.5ì´ˆ ì´ìƒ ì§€ì†ë˜ê³  ì•„ì§ ì•Œë¦¼ì„ ë³´ë‚´ì§€ ì•Šì•˜ë‹¤ë©´
                    if duration >= ALERT_DURATION_THRESHOLD and not detection_states[cls_name]['alert_sent']:
                        # ìƒí™© ë§¤í•‘
                        situation = SITUATION_MAPPING.get(cls_name, "UNKNOWN")
                        
                        # ìƒì„¸ ì •ë³´ ìƒì„±
                        detail = get_situation_detail(situation)
                        
                        # ì´ë¯¸ì§€ ì¸ì½”ë”©
                        image_base64 = encode_frame_to_base64(frame)
                        
                        # ì•Œë¦¼ ì „ì†¡
                        if send_alert_to_backend(situation, image_base64, None, None, detail):
                            detection_states[cls_name]['alert_sent'] = True
                            detection_states[cls_name]['alert_sent_time'] = current_time
                            print(f"âœ… ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ: {situation}")
                else:
                    # í´ë˜ìŠ¤ê°€ í˜„ì¬ íƒì§€ë˜ì§€ ì•ŠìŒ (ìƒíƒœ ì´ˆê¸°í™”)
                    detection_states[cls_name]['start_time'] = None
                
                # 1ë¶„ í›„ ì´ˆê¸°í™”
                if detection_states[cls_name]['alert_sent']:
                    if current_time - detection_states[cls_name]['alert_sent_time'] >= RESET_ALERT_AFTER:
                        detection_states[cls_name]['alert_sent'] = False
                        detection_states[cls_name]['alert_sent_time'] = None
                        print(f"[RESET] {cls_name} ì•Œë¦¼ ìƒíƒœ ì´ˆê¸°í™”")
            
            # ê²°ê³¼ í”„ë ˆì„ ì¶œë ¥
            cv2.imshow("AI Alert System", frame)

            # 'q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    except KeyboardInterrupt:
        print("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨.")
    
    finally:
        # ìì› í•´ì œ
        cap.release()
        cv2.destroyAllWindows()
        # client.disconnect()
        # if 'rtsp_proc' in locals():
        #     rtsp_proc.terminate()
        # if 'ffmpeg_proc' in locals():
        #     ffmpeg_proc.terminate()
        print("ğŸ¬ ì‹œìŠ¤í…œ ì¢…ë£Œ")

if __name__ == "__main__":
    main()
