{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d310cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "import cv2\n",
    "import torch\n",
    "import yaml\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from ultralytics import YOLO\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b90db",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICDES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75790ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 클래스 개수: 10\n",
      "최종 정의된 클래스: ['FLAMMABLE_MATERIAL', 'FOREIGN_OBJECT_WET', 'HANDLING_EQUIPMENT', 'IMPROPER_GEAR', 'INDIVIDUAL_MATERIAL', 'MATERIAL_COLLAPSE', 'SMOKING_ACTIVITY', 'SMOKING_VIOLATION', 'STACKED_MATERIAL', 'WORKER']\n",
      "최종 클래스 ID 매핑: {'FLAMMABLE_MATERIAL': 0, 'FOREIGN_OBJECT_WET': 1, 'HANDLING_EQUIPMENT': 2, 'IMPROPER_GEAR': 3, 'INDIVIDUAL_MATERIAL': 4, 'MATERIAL_COLLAPSE': 5, 'SMOKING_ACTIVITY': 6, 'SMOKING_VIOLATION': 7, 'STACKED_MATERIAL': 8, 'WORKER': 9}\n"
     ]
    }
   ],
   "source": [
    "# 원본 데이터셋 경로 설정 - 학습/검증 데이터가 이미 분리되어 있습니다.\n",
    "# 실제 경로로 반드시 변경해주세요!\n",
    "TRAIN_IMAGES_SOURCE_DIR = Path('C:/ssafy/AIoT-AI/Warehouse_Safety_Data/Training/images')\n",
    "TRAIN_LABELS_SOURCE_DIR = Path('C:/ssafy/AIoT-AI/Warehouse_Safety_Data/Training/labels')\n",
    "VAL_IMAGES_SOURCE_DIR = Path('C:/ssafy/AIoT-AI/Warehouse_Safety_Data/Validation/images')\n",
    "VAL_LABELS_SOURCE_DIR = Path('C:/ssafy/AIoT-AI/Warehouse_Safety_Data/Validation/labels')\n",
    "\n",
    "# 전처리된 YOLO 데이터셋이 저장될 최종 출력 경로\n",
    "OUTPUT_BASE_DIR = 'yolo_dataset'\n",
    "\n",
    "# 새로운 클래스 매핑을 요구사항에 맞춰 정의합니다.\n",
    "# 키(key)는 원본 클래스 ID이며, 값(value)은 새롭고 통합된 영어 레이블입니다.\n",
    "# class_mapping에 키로 존재하지 않는 원본 클래스 ID는 모두 데이터셋에서 제외됩니다.\n",
    "class_mapping = {\n",
    "    \"WO-01\": \"WORKER\",                # 작업자(작업복 착용)\n",
    "    \"WO-02\": \"WORKER\",                # 작업자(작업복 미 착용)\n",
    "    \"WO-03\": \"HANDLING_EQUIPMENT\",    # 화물트럭 -> 화물운반기기\n",
    "    \"WO-04\": \"HANDLING_EQUIPMENT\",    # 지게차 -> 화물운반기기\n",
    "    \"WO-05\": \"HANDLING_EQUIPMENT\",    # 핸드파레트카 -> 화물운반기기\n",
    "    \"WO-06\": \"HANDLING_EQUIPMENT\",    # 롤테이너 -> 화물운반기기\n",
    "    \"WO-07\": \"HANDLING_EQUIPMENT\",    # 운반수레 -> 화물운반기기\n",
    "    \"WO-08\": \"SMOKING_ACTIVITY\",      # 흡연\n",
    "    \"SO-02\": \"STACKED_MATERIAL\",      # 적재물류(그룹)\n",
    "    \"SO-03\": \"INDIVIDUAL_MATERIAL\",   # 물류(개별)\n",
    "    \"SO-21\": \"FOREIGN_OBJECT_WET\",    # 이물질(물,기름)\n",
    "    \"SO-22\": \"FLAMMABLE_MATERIAL\",    # 가연물,인화물(목재,섬유,석유통)\n",
    "    \"UA-06\": \"MATERIAL_COLLAPSE\",     # 물류 붕괴\n",
    "    \"UA-13\": \"MATERIAL_COLLAPSE\",     # 물류 붕괴\n",
    "    \"UA-07\": \"IMPROPER_GEAR\",         # 복장(안전모, 안전화)미착용\n",
    "    \"UA-20\": \"SMOKING_VIOLATION\"      # 비 흡연 구역 내 흡연\n",
    "}\n",
    "\n",
    "# class_mapping의 값들을 기반으로 최종 클래스 목록을 생성합니다.\n",
    "# 이를 통해 최종 목표 클래스만 포함되도록 합니다.\n",
    "all_classes = sorted(list(set(class_mapping.values())))\n",
    "class_to_id = {name: i for i, name in enumerate(all_classes)}\n",
    "id_to_class = {i: name for i, name in enumerate(all_classes)}\n",
    "\n",
    "print(f\"최종 클래스 개수: {len(all_classes)}\")\n",
    "print(f\"최종 정의된 클래스: {all_classes}\")\n",
    "print(f\"최종 클래스 ID 매핑: {class_to_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afe69683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'yolo_dataset'에 최종 YOLO 데이터셋 디렉토리 구조 생성 중...\n",
      "\n",
      "--- 학습 데이터 전처리 및 복사 ---\n",
      "'C:\\ssafy\\AIoT-AI\\Warehouse_Safety_Data\\Training\\labels'에서 JSON 파일 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33371/33371 [04:45<00:00, 116.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료. 처리된 파일: 33365, 건너뛴 파일: 6\n",
      "\n",
      "--- 검증 데이터 전처리 및 복사 ---\n",
      "'C:\\ssafy\\AIoT-AI\\Warehouse_Safety_Data\\Validation\\labels'에서 JSON 파일 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4172/4172 [01:11<00:00, 58.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료. 처리된 파일: 4172, 건너뛴 파일: 0\n",
      "\n",
      "최종 YOLO 데이터셋 통계:\n",
      "학습 이미지 수: 33365\n",
      "검증 이미지 수: 4172\n",
      "\n",
      "데이터셋 전처리가 완료되었습니다. 'yolo_dataset'에 YOLO 데이터셋이 생성되었습니다.\n",
      "학습을 위한 'dataset.yaml' 파일이 생성되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_json_to_yolo(json_file_path, output_dir, image_width, image_height, class_mapping, class_to_id):\n",
    "    \"\"\"\n",
    "    단일 JSON 레이블 파일을 YOLO 형식(.txt)으로 변환합니다.\n",
    "    클래스 재매핑 및 제외 로직이 적용됩니다.\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    image_id = Path(json_file_path).stem # 파일명에서 확장자 제거\n",
    "\n",
    "    yolo_labels = []\n",
    "    \n",
    "    if \"Learning data info.\" in data and \"annotation\" in data[\"Learning data info.\"]:\n",
    "        for annotation in data[\"Learning data info.\"][\"annotation\"]:\n",
    "            original_class_name = annotation[\"class_id\"]\n",
    "            \n",
    "            # 클래스 매핑 적용\n",
    "            mapped_class_name = class_mapping.get(original_class_name)\n",
    "\n",
    "            # 새로운 매핑에 정의되지 않은 (즉, 제외된) 클래스인 경우 건너뛰기\n",
    "            if mapped_class_name is None:\n",
    "                continue\n",
    "\n",
    "            # 매핑된 클래스 이름이 최종 class_to_id 목록에 있는지 확인\n",
    "            if mapped_class_name not in class_to_id:\n",
    "                print(f\"경고: 매핑된 클래스 '{mapped_class_name}' (원본 '{original_class_name}'에서)가 최종 정의된 클래스에서 발견되지 않았습니다. {json_file_path}의 어노테이션을 건너뜁니다.\")\n",
    "                continue\n",
    "\n",
    "            class_id = class_to_id[mapped_class_name]\n",
    "            coords = annotation[\"coord\"]\n",
    "            \n",
    "            # 'box' 타입 어노테이션 처리\n",
    "            if annotation[\"type\"] == \"box\":\n",
    "                if len(coords) == 4:\n",
    "                    x_min, y_min, width, height = coords\n",
    "                    \n",
    "                    # YOLO 형식에 맞는 정규화된 center_x, center_y, width, height 계산\n",
    "                    center_x = (x_min + width / 2) / image_width\n",
    "                    center_y = (y_min + height / 2) / image_height\n",
    "                    normalized_width = width / image_width\n",
    "                    normalized_height = height / image_height\n",
    "\n",
    "                    yolo_labels.append(f\"{class_id} {center_x:.6f} {center_y:.6f} {normalized_width:.6f} {normalized_height:.6f}\")\n",
    "                else:\n",
    "                    print(f\"경고: {json_file_path}의 박스 어노테이션에 예상치 못한 좌표 개수가 있습니다: {len(coords)}. 건너뜁니다.\")\n",
    "            # 폴리곤 어노테이션은 현재 건너뜁니다.\n",
    "            elif annotation[\"type\"] == \"polygon\":\n",
    "                print(f\"경고: 클래스 '{original_class_name}'에 대한 폴리곤 어노테이션이 발견되었습니다. 현재는 박스 어노테이션만 처리합니다. 건너뜁니다.\")\n",
    "                continue\n",
    "\n",
    "    output_txt_path = Path(output_dir) / f\"{image_id}.txt\"\n",
    "    # 쓸 레이블이 있는 경우에만 .txt 파일을 생성합니다.\n",
    "    if yolo_labels:\n",
    "        with open(output_txt_path, 'w', encoding='utf-8') as f:\n",
    "            for line in yolo_labels:\n",
    "                f.write(line + '\\n')\n",
    "        return True\n",
    "    else:\n",
    "        # 이 경우, 유효한 레이블이 없어 해당 이미지와 레이블을 최종 데이터셋에 포함하지 않습니다.\n",
    "        return False\n",
    "\n",
    "\n",
    "def process_and_copy_split_dataset(source_images_dir, source_labels_dir, target_images_dir, target_labels_dir, class_mapping, class_to_id):\n",
    "    \"\"\"\n",
    "    분할된 원본 데이터셋(이미지 및 JSON 레이블)을 YOLO 형식으로 변환하여 대상 디렉토리로 복사합니다.\n",
    "    \"\"\"\n",
    "    Path(target_images_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(target_labels_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    json_files = list(source_labels_dir.glob('*.json'))\n",
    "    \n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    print(f\"'{source_labels_dir}'에서 JSON 파일 처리 중...\")\n",
    "    for json_file in tqdm(json_files):\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            if \"Source data Info.\" in json_data and \"source_data_ID\" in json_data[\"Source data Info.\"]:\n",
    "                image_id = json_data[\"Source data Info.\"][\"source_data_ID\"]\n",
    "                image_extension = json_data[\"Source data Info.\"].get(\"file_extension\", \"jpg\")\n",
    "                image_filename = f\"{image_id}.{image_extension}\"\n",
    "                image_path = source_images_dir / image_filename\n",
    "\n",
    "                if not image_path.exists():\n",
    "                    print(f\"경고: {json_file}에 대한 이미지 파일을 찾을 수 없습니다. 건너뜁니다: {image_path}\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                if \"Raw data Info.\" in json_data and \"resolution\" in json_data[\"Raw data Info.\"]:\n",
    "                    width, height = json_data[\"Raw data Info.\"][\"resolution\"]\n",
    "                else:\n",
    "                    try:\n",
    "                        img = Image.open(image_path)\n",
    "                        width, height = img.size\n",
    "                    except Exception as e:\n",
    "                        print(f\"경고: {image_path}의 이미지 해상도를 읽는 중 오류 발생: {e}. 건너뜁니다.\")\n",
    "                        skipped_count += 1\n",
    "                        continue\n",
    "\n",
    "                # YOLO 형식으로 변환. 유효한 레이블이 생성된 경우에만 이미지와 레이블을 복사.\n",
    "                if convert_json_to_yolo(json_file, target_labels_dir, width, height, class_mapping, class_to_id):\n",
    "                    shutil.copy(image_path, target_images_dir / image_path.name)\n",
    "                    processed_count += 1\n",
    "                else:\n",
    "                    # 유효한 레이블이 없거나, 모든 레이블이 제외되어 빈 .txt 파일이 생성되지 않은 경우\n",
    "                    # 해당 이미지는 최종 데이터셋에 포함하지 않습니다.\n",
    "                    skipped_count += 1\n",
    "                    # print(f\"정보: 매핑/제외 후 {json_file}에 유효한 레이블이 없습니다. 이미지를 건너뜁니다.\")\n",
    "\n",
    "            else:\n",
    "                print(f\"경고: {json_file}에서 'Source data Info.source_data_ID'를 찾을 수 없습니다. 건너뜁니다.\")\n",
    "                skipped_count += 1\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"오류: {json_file} JSON 파일 디코딩 중 오류 발생: {e}. 건너뜁니다.\")\n",
    "            skipped_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"오류: {json_file} 처리 중 예상치 못한 오류 발생: {e}. 건너뜁니다.\")\n",
    "            skipped_count += 1\n",
    "    \n",
    "    print(f\"완료. 처리된 파일: {processed_count}, 건너뛴 파일: {skipped_count}\")\n",
    "    return processed_count\n",
    "\n",
    "\n",
    "def prepare_yolo_dataset(output_base_dir, class_mapping, class_to_id):\n",
    "    \"\"\"\n",
    "    미리 분할된 원본 데이터셋을 YOLO 학습에 적합한 디렉토리 구조로 변환하고 복사합니다.\n",
    "    \"\"\"\n",
    "    output_images_train_dir = Path(output_base_dir) / 'images' / 'train'\n",
    "    output_labels_train_dir = Path(output_base_dir) / 'labels' / 'train'\n",
    "    output_images_val_dir = Path(output_base_dir) / 'images' / 'val'\n",
    "    output_labels_val_dir = Path(output_base_dir) / 'labels' / 'val'\n",
    "\n",
    "    # 기존 출력 디렉토리 정리 및 재생성\n",
    "    if Path(output_base_dir).exists():\n",
    "        shutil.rmtree(output_base_dir)\n",
    "    \n",
    "    print(f\"'{output_base_dir}'에 최종 YOLO 데이터셋 디렉토리 구조 생성 중...\")\n",
    "    output_images_train_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_labels_train_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_images_val_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_labels_val_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"\\n--- 학습 데이터 전처리 및 복사 ---\")\n",
    "    train_processed_count = process_and_copy_split_dataset(\n",
    "        TRAIN_IMAGES_SOURCE_DIR, TRAIN_LABELS_SOURCE_DIR,\n",
    "        output_images_train_dir, output_labels_train_dir,\n",
    "        class_mapping, class_to_id\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- 검증 데이터 전처리 및 복사 ---\")\n",
    "    val_processed_count = process_and_copy_split_dataset(\n",
    "        VAL_IMAGES_SOURCE_DIR, VAL_LABELS_SOURCE_DIR,\n",
    "        output_images_val_dir, output_labels_val_dir,\n",
    "        class_mapping, class_to_id\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n최종 YOLO 데이터셋 통계:\")\n",
    "    print(f\"학습 이미지 수: {train_processed_count}\")\n",
    "    print(f\"검증 이미지 수: {val_processed_count}\")\n",
    "\n",
    "    # dataset.yaml 파일 생성\n",
    "    data_yaml = {\n",
    "        'path': str(Path(output_base_dir).resolve()),\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'nc': len(all_classes),\n",
    "        'names': all_classes\n",
    "    }\n",
    "\n",
    "    with open(Path(output_base_dir) / 'dataset.yaml', 'w') as f:\n",
    "        yaml.dump(data_yaml, f, sort_keys=False)\n",
    "\n",
    "    print(f\"\\n데이터셋 전처리가 완료되었습니다. '{output_base_dir}'에 YOLO 데이터셋이 생성되었습니다.\")\n",
    "    print(f\"학습을 위한 'dataset.yaml' 파일이 생성되었습니다.\")\n",
    "\n",
    "# 데이터 전처리 실행\n",
    "prepare_yolo_dataset(OUTPUT_BASE_DIR, class_mapping, class_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2370a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=40):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b3a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt to 'yolov8m.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49.7M/49.7M [00:11<00:00, 4.66MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.168  Python-3.10.18 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4050 Laptop GPU, 6141MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=yolo_dataset/dataset.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8m.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8m_warehouse_safety_finetune_split_data, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\yolov8m_warehouse_safety_finetune_split_data, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to 'C:\\Users\\SSAFY\\AppData\\Roaming\\Ultralytics\\Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 755k/755k [00:00<00:00, 7.81MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3781486  ultralytics.nn.modules.head.Detect           [10, [192, 384, 576]]         \n",
      "Model summary: 169 layers, 25,862,110 parameters, 25,862,094 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.35M/5.35M [00:01<00:00, 4.49MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1005.8652.1 MB/s, size: 928.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\ssafy\\AIoT-AI\\S13P11D103\\AI\\yolo_dataset\\labels\\train... 33365 images, 0 backgrounds, 0 corrupt: 100%|██████████| 33365/33365 [00:20<00:00, 1661.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\ssafy\\AIoT-AI\\S13P11D103\\AI\\yolo_dataset\\labels\\train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\anaconda3\\envs\\archieve\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1044.1340.1 MB/s, size: 936.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\ssafy\\AIoT-AI\\S13P11D103\\AI\\yolo_dataset\\labels\\val... 4172 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4172/4172 [00:02<00:00, 1681.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\ssafy\\AIoT-AI\\S13P11D103\\AI\\yolo_dataset\\labels\\val.cache\n",
      "Plotting labels to runs\\detect\\yolov8m_warehouse_safety_finetune_split_data\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\yolov8m_warehouse_safety_finetune_split_data\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100      6.31G      1.092      3.936      1.137        198        640:   2%|▏         | 37/2086 [00:49<45:57,  1.35s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 'data': 생성된 dataset.yaml 파일 경로\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 'epochs': 학습 에포크 수 (데이터셋 크기와 원하는 성능에 따라 조절)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 'imgsz': 모델의 입력 이미지 크기\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 'batch': 배치 크기 (GPU 메모리에 따라 조절)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 'name': 'runs/detect/' 아래에 결과가 저장될 폴더 이름\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mOUTPUT_BASE_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/dataset.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov8m_warehouse_safety_finetune_split_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\anaconda3\\envs\\archieve\\lib\\site-packages\\ultralytics\\engine\\model.py:799\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\anaconda3\\envs\\archieve\\lib\\site-packages\\ultralytics\\engine\\trainer.py:227\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\anaconda3\\envs\\archieve\\lib\\site-packages\\ultralytics\\engine\\trainer.py:419\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m     last_opt_step \u001b[38;5;241m=\u001b[39m ni\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;66;03m# Timed stopping\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\anaconda3\\envs\\archieve\\lib\\site-packages\\ultralytics\\engine\\trainer.py:642\u001b[0m, in \u001b[0;36mBaseTrainer.optimizer_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39munscale_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)  \u001b[38;5;66;03m# unscale gradients\u001b[39;00m\n\u001b[0;32m    641\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.0\u001b[39m)  \u001b[38;5;66;03m# clip gradients\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\anaconda3\\envs\\archieve\\lib\\site-packages\\torch\\amp\\grad_scaler.py:461\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    459\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 461\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    463\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\anaconda3\\envs\\archieve\\lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    349\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    354\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    356\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\anaconda3\\envs\\archieve\\lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    349\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    354\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    356\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 사전 학습된 YOLOv8m 모델 로드\n",
    "model = YOLO('yolov8m.pt')\n",
    "\n",
    "# 모델 학습\n",
    "# 'data': 생성된 dataset.yaml 파일 경로\n",
    "# 'epochs': 학습 에포크 수 (데이터셋 크기와 원하는 성능에 따라 조절)\n",
    "# 'imgsz': 모델의 입력 이미지 크기\n",
    "# 'batch': 배치 크기 (GPU 메모리에 따라 조절)\n",
    "# 'name': 'runs/detect/' 아래에 결과가 저장될 폴더 이름\n",
    "results = model.train(data=f'{OUTPUT_BASE_DIR}/dataset.yaml', epochs=100, imgsz=640, batch='auto', name='yolov8m_warehouse_safety_finetune_split_data', save_period=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66124e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_YAML_PATH = \"data.yaml\"\n",
    "\n",
    "with open(DATA_YAML_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data_yaml = yaml.safe_load(f)\n",
    "\n",
    "DATASET_PATH = os.path.dirname(DATA_YAML_PATH)\n",
    "TRAIN_IMAGES = os.path.join(DATASET_PATH, data_yaml[\"train\"].replace(\"../\", \"\"))\n",
    "VALID_IMAGES = os.path.join(DATASET_PATH, data_yaml[\"val\"].replace(\"../\", \"\"))\n",
    "\n",
    "print(DATASET_PATH)\n",
    "print(TRAIN_IMAGES)\n",
    "print(VALID_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ebebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adfc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObstacleDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_paths = glob(os.path.join(image_dir, \"*.jpg\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.image_paths[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        return self.transform(img) if self.transform else img\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_loader = DataLoader(ObstacleDataset(TRAIN_IMAGES, transform), batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(ObstacleDataset(VALID_IMAGES, transform), batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "def show_sample_images(image_loader):\n",
    "    sample_images = next(iter(image_loader))\n",
    "\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "    for i, img in enumerate(sample_images[:6]):\n",
    "        ax[i // 3, i % 3].imshow(img.permute(1, 2, 0).numpy())\n",
    "        ax[i // 3, i % 3].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"훈련 데이터 샘플\")\n",
    "show_sample_images(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de14b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"/kaggle/input/pothole-detection-challenge/train/images\"\n",
    "IMAGE_PATHS = sorted(glob(os.path.join(IMAGE_DIR, \"*.jpg\")))\n",
    "\n",
    "IMG_PATH = random.choice(IMAGE_PATHS)\n",
    "LABEL_PATH = IMG_PATH.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\")\n",
    "\n",
    "img = cv2.imread(IMG_PATH)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "h, w, _ = img.shape\n",
    "\n",
    "if os.path.exists(LABEL_PATH):\n",
    "    with open(LABEL_PATH, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            cls, cx, cy, bw, bh = map(float, line.strip().split())\n",
    "            x1 = int((cx - bw / 2) * w)\n",
    "            y1 = int((cy - bh / 2) * h)\n",
    "            x2 = int((cx + bw / 2) * w)\n",
    "            y2 = int((cy + bh / 2) * h)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 5)\n",
    "            cv2.putText(img, f\"Class {int(cls)}\", (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 1)\n",
    "else:\n",
    "    print(\"라벨 파일이 존재하지 않습니다:\", LABEL_PATH)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(os.path.basename(IMG_PATH))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"/kaggle/working/pothole_yolov8_train/weights/best.pt\")\n",
    "\n",
    "val_results = model.val(data=DATA_YAML_PATH, split=\"val\")\n",
    "\n",
    "print(\"검증 데이터 평가 결과:\")\n",
    "print(f\"mAP50: {val_results.box.map50:.4f}\")\n",
    "print(f\"mAP50-95: {val_results.box.map:.4f}\")\n",
    "print(f\"Precision: {val_results.box.mp:.4f}\")\n",
    "print(f\"Recall: {val_results.box.mr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5f4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbc3a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolov8m(data_path='data.yaml', epochs=100, img_size=640, batch_size=16, project='runs/train', name='exp'):\n",
    "    \"\"\"\n",
    "    Trains a YOLOv8m model.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the YAML file containing dataset configuration.\n",
    "        epochs (int): Number of training epochs.\n",
    "        img_size (int): Image size for training.\n",
    "        batch_size (int): Batch size for training.\n",
    "        project (str): Project name for saving results.\n",
    "        name (str): Experiment name for saving results within the project.\n",
    "    \"\"\"\n",
    "    # Load a YOLOv8m model\n",
    "    # You can load a pre-trained model for fine-tuning\n",
    "    model = YOLO('yolov8m.pt')  # yolov8m.pt is the pre-trained medium model\n",
    "\n",
    "    # If you want to train from scratch (less common for yolov8m, but possible)\n",
    "    # model = YOLO('yolov8m.yaml') # Create a new model from scratch configuration\n",
    "\n",
    "    print(f\"Starting YOLOv8m training with the following parameters:\")\n",
    "    print(f\"  Data: {data_path}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Image Size: {img_size}\")\n",
    "    print(f\"  Batch Size: {batch_size}\")\n",
    "    print(f\"  Project: {project}\")\n",
    "    print(f\"  Name: {name}\")\n",
    "\n",
    "    # Train the model\n",
    "    results = model.train(\n",
    "        data=data_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=img_size,\n",
    "        batch=batch_size,\n",
    "        project=project,\n",
    "        name=name,\n",
    "        # You can add more arguments here based on your needs, e.g.:\n",
    "        # device='0',         # GPU device (e.g., '0' for first GPU, or 'cpu')\n",
    "        # patience=50,        # Early stopping patience\n",
    "        # resume=False,       # Resume training from last checkpoint\n",
    "        # lr0=0.01,           # Initial learning rate\n",
    "        # lrf=0.001,          # Final learning rate (lr0 * lrf)\n",
    "        # optimizer='auto',   # Optimizer: 'SGD', 'Adam', 'AdamW', 'RMSProp', 'auto'\n",
    "        # workers=8,          # Number of DataLoader workers\n",
    "        # val=True,           # Validate during training\n",
    "        # cache=False,        # Cache images for faster training\n",
    "        # cos_lr=False,       # Cosine learning rate scheduler\n",
    "        # exist_ok=False,     # Don't overwrite existing results directory\n",
    "        # single_cls=False,   # Treat all classes as a single class\n",
    "        # save_period=10,     # Save checkpoint every 'save_period' epochs\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archieve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
